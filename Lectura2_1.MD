In this paper they focused on past user behavior without requiring the creation of explicit profiles. This work conducts an exploration into algorithms suitable for processing implicit feedback. They have analyzed watching habits of anonymized users. they will focus on models that are induced by Singular Value Decomposition (SVD) of the user-item observations matrix. They attempted to calculate value of confidence level and indicate the preference of user. Also, they proposed a scalable optimization procedure which scales linearly with the data size. Then they implemented their proposed algorithm that is used successfully within a recommender system for television shows. Finally, they compared favorably with well-tuned implementations with other known methods.
On the one hand, the author has focused on a user’s implicit feedback instead of explicit feed backs. In my opinion it is positive point because following such as purchase history, browsing patterns and mouse movements are more personalizer than following the rating that any user wants to give for a special item. Furthermore, most of people do not spend their time to rate for items, so following implicit feedbacks is easier than giving rates for items for users. Also it creates recommendations with high quality for users to find the best items.
In addition, they proposed a least squares model enables a novel way to compute explanations. It is another positive point for the articles. In view if the fact that finding and computing latent factors depend on another variable, their model reduces latent factor model into a linear mode that predicts preferences as a linear function of past actions weighted by item-item similarity.
I think that it is kind of merit that they considered some user’s implicit feed backs that if they did not consider on those, definitely their results could not be accurate for example maybe a TV program is showing but user is sleeping or It is likely that the initial show that was tuned into is of interest to the viewer, while the subsequent shows are of decreasing interest. So these cases prevent obtain real user’s preferences, they could overcome and assign this effect with a weighting. These were very interesting for me.
One of the another interesting feature in this paper for me the algorithm allows explaining the recommendations to the end user, which is a rarity among latent factor models by item-oriented neighborhood approach.
In general, they explained most of their approaches with details. It helps to readers to understand the article.
On the other hand, the authors did not study some challenges in recommended systems such as cold start in this paper. Moreover, although they toggled entries with rtui < 0.5 to zero and remove some values, in my opinion there are a lot of values yet that leads to problem of scalability. So it could be one subject that they can improve in future. 
Moreover, it is kind of demerit that the authors focused on calculation value of confidence level and the preference of user but they did not explain how they optimized their model. so, the part of optimazation was ambiguous for me.
It is interesting for me, in this paper they computed time complexity for some algorithms for example in part of their model, one of their computations time complexity is O(f2nu + f3). But in my opinion beside that they considered to it, O( f3) is not suitable time complexity for an algorithm. In fact, it is very high and leads to slow the speed of computations. So, I think that they should have considered this subject because the speed of recommendations time is very important too.

