
In this paper, they proposed the adaptation of a multi-armed bandit approach to dynamically optimize recommender system ensembles, by representing the combined systems as arms, and the ensemble as a bandit that at each step selects an arm to produce the next round of recommendations. During their work they considered illustrative pitfall examples such as overfitting behavior, self-defeating reinforcement loops, and poor decisions that can result from common, single-shot offline evaluation setups. Then they combined three recommendation algorithms such as KNN, matrix factorization and non-personalized most-popular recommendation to evaluate their bandit ensembles. After that they provided an environment to discover and rate the items for users as an offline evaluation approach. Finally, they showed that the bandit approach is empirically effective and improving for both of the individual algorithms and other ensemble alternatives.

It is positive point that in part of result they analyzed their result, in spite of the fact that all of papers that I have read they wrote some sentences with a lot of statistical that sometimes it makes me confused without any analysis.

However, they expressed that “bandit ensembles have been already used for A/B testing to select a winner between several candidates. Such winner is simply the algorithm that has been selected more times by the ensemble “. In my opinion, when a same winner is selected more times, practically it creates some biases. For example, imagine that most of the time there is an algorithm that is winner so there is not a condition to participate for other algorithms. So, the multi-armed bandit approach is approximately convert to an individual approach just for one algorithms. Therefore, the ensemble is strongly dependent on the same winner.

Moreover, one of the negative thing that they should have considered about that computational complexity (time Complexity) of their algorithms. There is a question in my mind about the adaptation of a multi-armed bandit approach that are selecting a winner does a winner have a good computational complexity? Because to optimize ensembles recommender system the value of time complexity is very important especially for the winner algorithm. They should have considered to it as a measure to select a winner.

Furthermore, we initialize the Thompson sampling with αa=1,000 and βa= 1 for all the three combined recommendation algorithms, and ε=0.1 for the ε-greedy bandit but they did not explain why they used αa=1,000 and βa= 1 ε=0.1?

However, it is interesting that each recommender system has its own training, test and exclusion sets, since they are built from the user feedback to their own specific recommendations. But since one epoch implies recommending an item for each user, the number of hits and misses can be in the order of thousands. So, in my opinion, it is negative point for the method because executing thousands of data is time-consuming for system. So, it can decelerate recommender system speed. However, using of thousands data with a multi-armed bandit approach can prevent scalability. Because as they expressed in their paper “important advantage of bandit ensembles is their low computational cost”. So, they need to run just one selected recommendation algorithm, and not all the others. Therefore, their method is powerful in terms of Computational power.
 

