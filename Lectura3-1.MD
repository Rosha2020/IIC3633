
In this paper, they had two data set such as Netflix and Movielens. They evaluated performance of several collaborative filtering algorithms via accuracy metrics to follow the Top-N recommendation for these two dataset. They explained that how divided their data set to training and test sets. Then their ratings splits of distribution of Popular items vs. long-tail. They compare some algorithms such as Asymmetric-SVD with non-personalized algorithm based on item popularity. They found that with RMSE, they can compare performance of the non-personalized algorithm on Top-N recommendations with, personalized algorithms. Then they added an additional set of experiments to non-personalized algorithm but they exclude the extremely popular items to evaluate the performances. Finally, they proposed two new kinds of collaborative filtering algorithms such as Neighborhood models and Latent Factor Models to improve recommender algorithms in Top-N recommendations but these new algorithms do not decrease RMSE.

On the one hand, it is kind of merit because they used Neighborhood models to predict on the similarity among item-item. They explained well why they focused on item-item neighborhood algorithms. However, to rank items has been used a coefficient denoted and then has been removed its denominator that it cannot be convinced. For example, they explained about benefits of absence of the denominator. So what will happen if it exists?

Another positive point of this paper is proposing Asymmetric-SVD. I think that this method could address some challenges such as cold start. It also can have a novelty to make recommendation systems for users. Because this method does not need to re-evaluation of parameters. Also they can immediately adjust their recommendations to just entered ratings, providing users with an immediate feedback for their actions.

The authors used recall and precision together to compute accuracy metrics. In my opinion it is kind of merit because using only recall to evaluate performance of system is not suitable. It should be used with precision. Because their model needs to have correct prediction with high value. So, it is positive point that they used these measures together. However, they could have used F1 measure. Because F1 can give more accurate evaluation on fp and tn.

On the other hand, the authors assumed that the test set contains only 5-stars for both of data sets. But they did not explain why do they assume just 5-stars to probe set. So, I think that it is a negative point because they ignored some realities in ratings with only 5-stars. They should have emphasized more efforts on real ratings
They used a formulize to compute the item-item similarity as the coefficient dij. There was a ʎ1 is 100. But they did not explain why the typical value of ʎ1. 
They told that Three of the algorithms – TopPop, NNCosNgbr, and PureSVD are not sensible from an error minimization viewpoint and cannot be assessed by an RMSE measure. But they did not explain why they ignored RMSE measure. In my opinin, they should have used RMSE measure because in the paper there was not outliers. So, it could be suitable measure to evaluate above algorithms and they could have more interesting result.

Another negative point is that in part of result they needed to have a table to show their result but they did not use tables.
