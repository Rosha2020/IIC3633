
Netflix provided 100M ratings (from 1 to 5) of 17K movies by 500K users. They used a triple of number that include (User, Movie, Rating). They tried the anticipate the value of Rating. To visualize, there are a big sparsely filled matrix with users on the top and movies the down side or vice versa. Each cell in the matrix either contains an observed rating (1-5) for that movie (row) by that user (column), or is blank. This matrix has 8.5 billion entries. Netflix has then posed a "quiz" which consists of a bunch of question marks and we have to fill best-guess ratings in their places. The measure of accuracy is mean squared error. Also they provide a date for both the ratings and the question marks and the matrix can potentially have more than one rating in it. In view of the fact that there 8.5 billion ratings and a lot of weary users, they needed to define a model with smaller number of parameters which is called singular value decomposition. Then, to compute the SVD we have two problems as follows: they do not have 8.5B entries and another problem is that how to compute SVD. They used some codes to explain work form of equations for the error between the SVD-like model and the original data then took the derivative with respect to the parameters. Finally, he used Gaussian distribution with two choices. One of them is to simply clip the prediction to the range 1-5 and another is to introduce some functional non-linearity such as a sigmoid. Then he compared the results of the probe and training rmse for the first few features with and without the regularization term ("decay") enabled.

One the one hand, he did programming with some codes. In my opinion those code has made easy to understand the methods that he has used. For example, static inline and predictRating().

they needed to define a model with smaller number of parameters which is called SVD. In my opinion, he considered to exceptions well. For example, what will happen if there is a user who has only rated one movie or if there's a movie which only appears in the training set once, with a rating of 1. 
On the other hand, the author used lrate = 0.001, but he did not explain that why Lrate is 0.001. for example, if we assume Lrate =0.002 what will happen? and he did not explain why it cannot be another value. 

Furthermore, the author considers to some exception, I have an idea what happen if a user watched a movie several times and rated several time for each time with different rating just for one movies? For example, user X watched movie Y twice then user x gets rating 4 to movie Y, after he watches twice movie Y and gets rating 2. In my opinion, he should have considered to this kind of exception and computed SVD for this.
In addition, the author used the sigmoid. Although he did not explain how to work this function, after googling, he worked the best was to simply fit a piecewise linear approximation to the true output/output curve. But as he said for the first feature for instance, you end up with a kink around the origin such that the impact of negative values is greater than the impact of positive ones. In addition, the sigmoid function has another problem that Dying Relu problem - if too many activations get below zero then most of the units(neurons) in network with Relu will simply output zero, in other words, die and thereby prohibiting learning. So, he should have used Leaky-Relu instead.

Moreover, he indicated some results on 3 figures and did not explain well the results. He should have explained some results with examples to make sense for readers similar to other papers.  


